{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9a8162-755a-4b23-af7c-2bb98f6ccd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to convert this notebook to .py if you want to run it via command line or with Slurm\n",
    "# from subprocess import call\n",
    "# command = \"jupyter nbconvert Reconstructions.ipynb --to python\"\n",
    "# call(command,shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d437e335-a02b-4383-8fd1-cc80641b483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import webdataset as wds\n",
    "import PIL\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "local_rank = 0\n",
    "print(\"device:\",device)\n",
    "\n",
    "import utils\n",
    "from models import Clipper, OpenClipper, BrainNetwork, BrainDiffusionPrior, BrainDiffusionPriorOld, Voxel2StableDiffusionModel, VersatileDiffusionPriorNetwork\n",
    "\n",
    "if utils.is_interactive():\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "\n",
    "seed=42\n",
    "utils.seed_everything(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8256451-9423-4860-bf18-f2e5d717b749",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89180d4-de6c-47e6-9acd-cad412eba029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--data_path=/fsx/proj-medarc/fmri/natural-scenes-dataset', '--subj=1', '--model_name=prior_257_final_subj01_bimixco_softclip_byol']\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    # Example use\n",
    "    jupyter_args = \"--data_path=/fsx/proj-medarc/fmri/natural-scenes-dataset \\\n",
    "                    --subj=1 \\\n",
    "                    --model_name=prior_257_final_subj01_bimixco_softclip_byol\"\n",
    "    \n",
    "    jupyter_args = jupyter_args.split()\n",
    "    print(jupyter_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da9bc894-8b43-4217-bce6-ef26d26e70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of trained model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--autoencoder_name\", type=str, default=\"None\",\n",
    "    help=\"name of trained autoencoder model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-medarc/fmri/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored (see README)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,5,7],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--img2img_strength\",type=float, default=.85,\n",
    "    help=\"How much img2img (1=no img2img; 0=outputting the low-level image itself)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--recons_per_sample\", type=int, default=1,\n",
    "    help=\"How many recons to output, to then automatically pick the best one (MindEye uses 16)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--vd_cache_dir\", type=str, default='/fsx/proj-medarc/fmri/cache/models--shi-labs--versatile-diffusion/snapshots/2926f8e11ea526b562cd592b099fcf9c2985d0b7',\n",
    "    help=\"Where is cached Versatile Diffusion model; if not cached will download to this path\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dropout_ratio\",type=float,default=0.15,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mcd_inference\", action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"Performs inference using Monte Carlo Dropout, creating a bayesian ensemble model to estimate uncertainty\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mcd_forward_passes\",type=int,default=10,\n",
    ")\n",
    "\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "if autoencoder_name==\"None\":\n",
    "    autoencoder_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b279bc-905f-4648-93ba-7bd79c34529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj 1 num_voxels 15724\n"
     ]
    }
   ],
   "source": [
    "if subj == 1:\n",
    "    num_voxels = 15724\n",
    "elif subj == 2:\n",
    "    num_voxels = 14278\n",
    "elif subj == 3:\n",
    "    num_voxels = 15226\n",
    "elif subj == 4:\n",
    "    num_voxels = 13153\n",
    "elif subj == 5:\n",
    "    num_voxels = 13039\n",
    "elif subj == 6:\n",
    "    num_voxels = 17907\n",
    "elif subj == 7:\n",
    "    num_voxels = 12682\n",
    "elif subj == 8:\n",
    "    num_voxels = 14386\n",
    "print(\"subj\",subj,\"num_voxels\",num_voxels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ab004a8-c98b-44ee-9bae-0e8eb9e2d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n",
      "voxel.shape torch.Size([1, 3, 15724])\n",
      "img_input.shape torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "val_url = f\"{data_path}/webdataset_avg_split/test/test_subj0{subj}_\" + \"{0..1}.tar\"\n",
    "meta_url = f\"{data_path}/webdataset_avg_split/metadata_subj0{subj}.json\"\n",
    "num_train = 8559 + 300\n",
    "num_val = 982\n",
    "batch_size = val_batch_size = 1\n",
    "voxels_key = 'nsdgeneral.npy' # 1d inputs\n",
    "\n",
    "val_data = wds.WebDataset(val_url, resampled=False)\\\n",
    "    .decode(\"torch\")\\\n",
    "    .rename(images=\"jpg;png\", voxels=voxels_key, trial=\"trial.npy\", coco=\"coco73k.npy\", reps=\"num_uniques.npy\")\\\n",
    "    .to_tuple(\"voxels\", \"images\", \"coco\")\\\n",
    "    .batched(val_batch_size, partial=False)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(val_data, batch_size=None, shuffle=False)\n",
    "\n",
    "# check that your data loader is working\n",
    "for val_i, (voxel, img_input, coco) in enumerate(val_dl):\n",
    "    print(\"idx\",val_i)\n",
    "    print(\"voxel.shape\",voxel.shape)\n",
    "    print(\"img_input.shape\",img_input.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10121a-87fa-4f4a-95d1-9b9d4f7c6a04",
   "metadata": {},
   "source": [
    "## Load autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7622029d-ea95-43fc-8688-81057dcadf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid path for low-level model specified; not using img2img!\n"
     ]
    }
   ],
   "source": [
    "from models import Voxel2StableDiffusionModel\n",
    "\n",
    "outdir = f'../train_logs/{autoencoder_name}'\n",
    "ckpt_path = os.path.join(outdir, f'epoch120.pth')\n",
    "\n",
    "if os.path.exists(ckpt_path):\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    voxel2sd = Voxel2StableDiffusionModel(in_dim=num_voxels)\n",
    "\n",
    "    voxel2sd.load_state_dict(state_dict,strict=False)\n",
    "    voxel2sd.eval()\n",
    "    voxel2sd.to(device)\n",
    "    print(\"Loaded low-level model!\")\n",
    "else:\n",
    "    print(\"No valid path for low-level model specified; not using img2img!\") \n",
    "    img2img_strength = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7a122-e411-4f7b-be47-aa91fb4e47ea",
   "metadata": {},
   "source": [
    "# Load VD pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8a793d-5b52-42e2-a6b7-d37beaf19da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating versatile diffusion reconstruction pipeline...\n"
     ]
    }
   ],
   "source": [
    "print('Creating versatile diffusion reconstruction pipeline...')\n",
    "from diffusers import VersatileDiffusionDualGuidedPipeline, UniPCMultistepScheduler\n",
    "from diffusers.models import DualTransformer2DModel\n",
    "try:\n",
    "    vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(vd_cache_dir).to(device)\n",
    "except:\n",
    "    print(\"Downloading Versatile Diffusion to\", vd_cache_dir)\n",
    "    vd_pipe =  VersatileDiffusionDualGuidedPipeline.from_pretrained(\n",
    "            \"shi-labs/versatile-diffusion\",\n",
    "            cache_dir = vd_cache_dir).to(device)\n",
    "vd_pipe.image_unet.eval()\n",
    "vd_pipe.vae.eval()\n",
    "vd_pipe.image_unet.requires_grad_(False)\n",
    "vd_pipe.vae.requires_grad_(False)\n",
    "\n",
    "vd_pipe.scheduler = UniPCMultistepScheduler.from_pretrained(vd_cache_dir, subfolder=\"scheduler\")\n",
    "num_inference_steps = 20\n",
    "\n",
    "# Set weighting of Dual-Guidance \n",
    "text_image_ratio = .0 # .5 means equally weight text and image, 0 means use only image\n",
    "for name, module in vd_pipe.image_unet.named_modules():\n",
    "    if isinstance(module, DualTransformer2DModel):\n",
    "        module.mix_ratio = text_image_ratio\n",
    "        for i, type in enumerate((\"text\", \"image\")):\n",
    "            if type == \"text\":\n",
    "                module.condition_lengths[i] = 77\n",
    "                module.transformer_index_for_condition[i] = 1  # use the second (text) transformer\n",
    "            else:\n",
    "                module.condition_lengths[i] = 257\n",
    "                module.transformer_index_for_condition[i] = 0  # use the first (image) transformer\n",
    "\n",
    "unet = vd_pipe.image_unet\n",
    "vae = vd_pipe.vae\n",
    "noise_scheduler = vd_pipe.scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403bb740-db7a-4b1d-9c2b-2c78a9ace10d",
   "metadata": {},
   "source": [
    "## Load Versatile Diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c427f9b1-afbe-43df-a0bc-8c28842c7f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda\n",
      "ckpt_path ../train_logs/prior_257_final_subj01_bimixco_softclip_byol/last.pth\n",
      "EPOCH:  239\n"
     ]
    }
   ],
   "source": [
    "img_variations = False\n",
    "\n",
    "out_dim = 257 * 768\n",
    "clip_extractor = Clipper(\"ViT-L/14\", hidden_state=True, norm_embs=True, device=device)\n",
    "voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "voxel2clip.requires_grad_(False)\n",
    "voxel2clip.eval()\n",
    "\n",
    "out_dim = 768\n",
    "depth = 6\n",
    "dim_head = 64\n",
    "heads = 12 # heads * dim_head = 12 * 64 = 768\n",
    "timesteps = 100 #100\n",
    "\n",
    "prior_network = VersatileDiffusionPriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    "    voxel2clip=voxel2clip,\n",
    ")\n",
    "\n",
    "outdir = f'../train_logs/{model_name}'\n",
    "ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "print(\"ckpt_path\",ckpt_path)\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "print(\"EPOCH: \",checkpoint['epoch'])\n",
    "diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "diffusion_prior.eval().to(device)\n",
    "diffusion_priors = [diffusion_prior]\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8940c9cb-5bf7-4381-8e2e-096cbc381e4a",
   "metadata": {},
   "source": [
    "## Load Image Variations model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc1b0710-78a5-4d07-8940-363072eaa789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_variations = True\n",
    "\n",
    "# # CLS model\n",
    "# out_dim = 768\n",
    "# clip_extractor = Clipper(\"ViT-L/14\", hidden_state=False, norm_embs=False, device=device)\n",
    "# voxel2clip_kwargs = dict(in_dim=num_voxels,out_dim=out_dim)\n",
    "# voxel2clip = BrainNetwork(**voxel2clip_kwargs)\n",
    "# voxel2clip.requires_grad_(False)\n",
    "# voxel2clip.eval()\n",
    "\n",
    "# diffusion_prior = BrainDiffusionPriorOld.from_pretrained(\n",
    "#     # kwargs for DiffusionPriorNetwork\n",
    "#     dict(),\n",
    "#     # kwargs for DiffusionNetwork\n",
    "#     dict(\n",
    "#         condition_on_text_encodings=False,\n",
    "#         timesteps=1000,\n",
    "#         voxel2clip=voxel2clip,\n",
    "#     ),\n",
    "#     voxel2clip_path=None,\n",
    "# )\n",
    "\n",
    "# outdir = f'../train_logs/{model_name}'\n",
    "# ckpt_path = os.path.join(outdir, f'last.pth')\n",
    "\n",
    "# print(\"ckpt_path\",ckpt_path)\n",
    "# checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "# state_dict = checkpoint['model_state_dict']\n",
    "# print(\"EPOCH: \",checkpoint['epoch'])\n",
    "# diffusion_prior.load_state_dict(state_dict,strict=False)\n",
    "# diffusion_prior.eval().to(device)\n",
    "# diffusion_priors = [diffusion_prior]\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e65940-f62f-4091-bc9a-faeb545bc18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import AutoencoderKL, UNet2DConditionModel, UniPCMultistepScheduler\n",
    "\n",
    "# sd_cache_dir = '/fsx/home-paulscotti/.cache/huggingface/diffusers/models--lambdalabs--sd-image-variations-diffusers/snapshots/a2a13984e57db80adcc9e3f85d568dcccb9b29fc'\n",
    "# unet = UNet2DConditionModel.from_pretrained(sd_cache_dir,subfolder=\"unet\").to(device)\n",
    "\n",
    "# unet.eval() # dont want to train model\n",
    "# unet.requires_grad_(False) # dont need to calculate gradients\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(sd_cache_dir,subfolder=\"vae\").to(device)\n",
    "# vae.eval()\n",
    "# vae.requires_grad_(False)\n",
    "\n",
    "# noise_scheduler = UniPCMultistepScheduler.from_pretrained(sd_cache_dir, subfolder=\"scheduler\")\n",
    "# num_inference_steps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d66a1c-cb99-4484-8789-4b9f33d1f994",
   "metadata": {},
   "source": [
    "# Reconstruct one-at-a-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7d4d45-6a22-4150-a987-285f2f630a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "retrieve = False\n",
    "plotting = False\n",
    "saving = True\n",
    "verbose = False\n",
    "imsize = 512\n",
    "\n",
    "if img_variations:\n",
    "    guidance_scale = 7.5\n",
    "else:\n",
    "    guidance_scale = 3.5\n",
    "    \n",
    "ind_include = np.arange(num_val)\n",
    "all_brain_recons = None\n",
    "    \n",
    "only_lowlevel = False\n",
    "if img2img_strength == 1:\n",
    "    img2img = False\n",
    "elif img2img_strength == 0:\n",
    "    img2img = True\n",
    "    only_lowlevel = True\n",
    "else:\n",
    "    img2img = True\n",
    "    \n",
    "for val_i, (voxel, img, coco) in enumerate(tqdm(val_dl,total=len(ind_include))):\n",
    "    if val_i<np.min(ind_include):\n",
    "        continue\n",
    "    voxel = torch.mean(voxel,axis=1).to(device)\n",
    "    # voxel = voxel[:,0].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if img2img:\n",
    "            ae_preds = voxel2sd(voxel.float())\n",
    "            blurry_recons = vd_pipe.vae.decode(ae_preds.to(device).half()/0.18215).sample / 2 + 0.5\n",
    "\n",
    "            if val_i==0:\n",
    "                plt.imshow(utils.torch_to_Image(blurry_recons))\n",
    "                plt.show()\n",
    "        else:\n",
    "            blurry_recons = None\n",
    "\n",
    "        if only_lowlevel:\n",
    "            brain_recons = blurry_recons\n",
    "        else:\n",
    "            if mcd_inference:\n",
    "                grid, brain_recons, laion_best_picks, recon_img, extracted_clip, mc_variance, mc_entropy, mc_mutual_info = utils.reconstruction_mcd(\n",
    "                img, voxel,\n",
    "                clip_extractor, unet, vae, noise_scheduler,\n",
    "                voxel2clip_cls = None, #diffusion_prior_cls.voxel2clip,\n",
    "                diffusion_priors = diffusion_priors,\n",
    "                text_token = None,\n",
    "                img_lowlevel = blurry_recons,\n",
    "                num_inference_steps = num_inference_steps,\n",
    "                n_samples_save = batch_size,\n",
    "                recons_per_sample = recons_per_sample,\n",
    "                guidance_scale = guidance_scale,\n",
    "                img2img_strength = img2img_strength, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "                timesteps_prior = 100,\n",
    "                seed = seed,\n",
    "                retrieve = retrieve,\n",
    "                plotting = plotting,\n",
    "                img_variations = img_variations,\n",
    "                verbose = verbose,\n",
    "                mcd_forward_passes=mcd_forward_passes,\n",
    "            )\n",
    "            else:\n",
    "                grid, brain_recons, laion_best_picks, recon_img = utils.reconstruction(\n",
    "                    img, voxel,\n",
    "                    clip_extractor, unet, vae, noise_scheduler,\n",
    "                    voxel2clip_cls = None, #diffusion_prior_cls.voxel2clip,\n",
    "                    diffusion_priors = diffusion_priors,\n",
    "                    text_token = None,\n",
    "                    img_lowlevel = blurry_recons,\n",
    "                    num_inference_steps = num_inference_steps,\n",
    "                    n_samples_save = batch_size,\n",
    "                    recons_per_sample = recons_per_sample,\n",
    "                    guidance_scale = guidance_scale,\n",
    "                    img2img_strength = img2img_strength, # 0=fully rely on img_lowlevel, 1=not doing img2img\n",
    "                    timesteps_prior = 100,\n",
    "                    seed = seed,\n",
    "                    retrieve = retrieve,\n",
    "                    plotting = plotting,\n",
    "                    img_variations = img_variations,\n",
    "                    verbose = verbose,\n",
    "                )\n",
    "\n",
    "            if plotting:\n",
    "                plt.show()\n",
    "                # grid.savefig(f'evals/{model_name}_{val_i}.png')\n",
    "\n",
    "            brain_recons = brain_recons[:,laion_best_picks.astype(np.int8)]\n",
    "\n",
    "        if all_brain_recons is None:\n",
    "            all_brain_recons = brain_recons\n",
    "            all_images = img\n",
    "        else:\n",
    "            all_brain_recons = torch.vstack((all_brain_recons,brain_recons))\n",
    "            all_images = torch.vstack((all_images,img))\n",
    "\n",
    "    if val_i>=np.max(ind_include):\n",
    "        break\n",
    "\n",
    "all_brain_recons = all_brain_recons.view(-1,3,imsize,imsize)\n",
    "print(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "if saving:\n",
    "    torch.save(all_images,f'all_images.pt')\n",
    "    torch.save(all_brain_recons,f'{model_name}_recons_img2img{img2img_strength}_{recons_per_sample}samples.pt')\n",
    "print(f'recon_path: {model_name}_recons_img2img{img2img_strength}_{recons_per_sample}samples')\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
